{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13029135,"sourceType":"datasetVersion","datasetId":8249856}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# /kaggle/input/vllm-dataset/images/aut-0003-00003809.jpg   All images in this directory called images \n# /kaggle/input/vllm-dataset/dataset.jsonl                  all associated prompts are in this file \n\n# Dataset.jsonl file data strcuture \n\n# example \n\"root\":{2 items\n\"image\":string\"images/aut-0003-00003809.jpg\"\n\"conversations\":[2 items\n0:{2 items\n\"from\":string\"human\"\n\"value\":string\"<image>\"\n}\n1:{2 items\n\"from\":string\"gpt\"\n\"value\":string\"The image depicts an airport scene with a clear blue sky scattered with white clouds. The runway is wet, reflecting the sunlight, indicating recent rain or ongoing maintenance. The runway is marked with white lines for navigation and safety. There are several vehicles on the runway, including: - 9 Standard Cars (2 in the left far, 6 in the center far, 1 in the right far) - 1 Taxiing Vehicle (1 in the right far) - 1 Passenger Jet (1 in the center far) - 2 Business Jets (2 in the left far) The airport has a large hangar structure to the right, which appears to be a hangar for aircraft storage. The hangar has a grayish exterior with windows and doors. There are also other buildings and structures visible in the background, including a tower with a red and white striped flag. Overall, the scene suggests a busy airport with various types of aircraft and vehicles in operation.\"\n}\n]\n}","metadata":{"execution":{"iopub.status.busy":"2025-09-11T12:47:06.093545Z","iopub.execute_input":"2025-09-11T12:47:06.095073Z","iopub.status.idle":"2025-09-11T12:47:06.104500Z","shell.execute_reply.started":"2025-09-11T12:47:06.095014Z","shell.execute_reply":"2025-09-11T12:47:06.102741Z"}}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom transformers import (\n    Qwen2VLForConditionalGeneration,\n    AutoTokenizer,\n    AutoProcessor,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForSeq2Seq\n)\nfrom datasets import Dataset as HFDataset\nimport pandas as pd\nfrom sklearn.metrics import bleu_score\nfrom torchvision import transforms\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass VisionLanguageDataset(Dataset):\n    \"\"\"Custom dataset for vision-language pairs\"\"\"\n    \n    def __init__(self, data_dir, processor, max_length=512):\n        self.data_dir = data_dir\n        self.processor = processor\n        self.max_length = max_length\n        \n        # Load dataset - assuming structure: images/ and prompts.json or similar\n        self.data = self.load_data()\n        \n    def load_data(self):\n        \"\"\"Load image-text pairs from directory structure\"\"\"\n        data = []\n        \n        # Method 1: JSON file with image-prompt pairs\n        json_path = os.path.join(self.data_dir, 'data.json')\n        if os.path.exists(json_path):\n            with open(json_path, 'r') as f:\n                data = json.load(f)\n        \n        # Method 2: Separate images and prompts directories\n        elif os.path.exists(os.path.join(self.data_dir, 'images')) and \\\n             os.path.exists(os.path.join(self.data_dir, 'prompts.txt')):\n            \n            with open(os.path.join(self.data_dir, 'prompts.txt'), 'r') as f:\n                prompts = f.readlines()\n            \n            image_dir = os.path.join(self.data_dir, 'images')\n            image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n            \n            for i, (img_file, prompt) in enumerate(zip(image_files, prompts)):\n                data.append({\n                    'image_path': os.path.join(image_dir, img_file),\n                    'prompt': prompt.strip(),\n                    'id': i\n                })\n        \n        # Method 3: Auto-discover paired files\n        else:\n            for root, dirs, files in os.walk(self.data_dir):\n                for file in files:\n                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        img_path = os.path.join(root, file)\n                        # Look for corresponding text file\n                        txt_path = img_path.rsplit('.', 1)[0] + '.txt'\n                        if os.path.exists(txt_path):\n                            with open(txt_path, 'r') as f:\n                                prompt = f.read().strip()\n                            data.append({\n                                'image_path': img_path,\n                                'prompt': prompt,\n                                'id': len(data)\n                            })\n        \n        print(f\"Loaded {len(data)} image-text pairs\")\n        return data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        # Load and process image\n        try:\n            image = Image.open(item['image_path']).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {item['image_path']}: {e}\")\n            # Return a blank image as fallback\n            image = Image.new('RGB', (224, 224), color='white')\n        \n        # Process with Qwen processor\n        inputs = self.processor(\n            text=item['prompt'],\n            images=image,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=self.max_length\n        )\n        \n        # Flatten tensors\n        for key in inputs:\n            if inputs[key].dim() > 1:\n                inputs[key] = inputs[key].squeeze(0)\n        \n        inputs['labels'] = inputs['input_ids'].clone()\n        inputs['image_path'] = item['image_path']\n        inputs['original_prompt'] = item['prompt']\n        \n        return inputs\n\nclass QwenVLTrainer:\n    \"\"\"Main trainer class for Qwen VL fine-tuning\"\"\"\n    \n    def __init__(self, model_name=\"Qwen/Qwen2-VL-2B-Instruct\", data_dir=\"./data\"):\n        self.model_name = model_name\n        self.data_dir = data_dir\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Initialize model and processor\n        self.model = None\n        self.processor = None\n        self.tokenizer = None\n        self.load_model()\n        \n        # Training metrics\n        self.train_losses = []\n        self.eval_losses = []\n        self.bleu_scores = []\n        \n    def load_model(self):\n        \"\"\"Load Qwen VL model and processor\"\"\"\n        try:\n            print(\"Loading Qwen VL model...\")\n            self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n                self.model_name,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                device_map=\"auto\" if torch.cuda.is_available() else None\n            )\n            \n            self.processor = AutoProcessor.from_pretrained(self.model_name)\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n            \n            if self.tokenizer.pad_token is None:\n                self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            print(f\"Model loaded successfully on {self.device}\")\n            \n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            print(\"Using a simpler configuration...\")\n            # Fallback configuration\n            \n    def prepare_datasets(self, train_ratio=0.8, val_ratio=0.1):\n        \"\"\"Prepare train, validation, and test datasets\"\"\"\n        \n        # Create full dataset\n        full_dataset = VisionLanguageDataset(self.data_dir, self.processor)\n        \n        # Split dataset\n        total_size = len(full_dataset)\n        train_size = int(train_ratio * total_size)\n        val_size = int(val_ratio * total_size)\n        test_size = total_size - train_size - val_size\n        \n        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n            full_dataset, [train_size, val_size, test_size]\n        )\n        \n        print(f\"Dataset splits - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n        \n        return train_dataset, val_dataset, test_dataset\n    \n    def train(self, train_dataset, val_dataset, output_dir=\"./qwen_vl_finetuned\", \n              num_epochs=3, batch_size=4, learning_rate=2e-5):\n        \"\"\"Train the model\"\"\"\n        \n        # Data collator\n        data_collator = DataCollatorForSeq2Seq(\n            tokenizer=self.tokenizer,\n            model=self.model,\n            padding=True\n        )\n        \n        # Training arguments\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=num_epochs,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=batch_size,\n            warmup_steps=100,\n            weight_decay=0.01,\n            learning_rate=learning_rate,\n            logging_dir=f'{output_dir}/logs',\n            logging_steps=10,\n            evaluation_strategy=\"steps\",\n            eval_steps=100,\n            save_steps=500,\n            save_total_limit=3,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_loss\",\n            greater_is_better=False,\n            report_to=None,  # Disable wandb/tensorboard\n            dataloader_pin_memory=False,\n            fp16=torch.cuda.is_available(),\n        )\n        \n        # Initialize trainer\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset,\n            tokenizer=self.tokenizer,\n            data_collator=data_collator,\n        )\n        \n        # Train the model\n        print(\"Starting training...\")\n        trainer.train()\n        \n        # Save the final model\n        trainer.save_model()\n        print(f\"Model saved to {output_dir}\")\n        \n        return trainer\n    \n    def generate_prediction(self, image_path, prompt=\"Describe this image:\"):\n        \"\"\"Generate prediction for a single image\"\"\"\n        try:\n            image = Image.open(image_path).convert('RGB')\n            \n            # Process inputs\n            inputs = self.processor(\n                text=prompt,\n                images=image,\n                return_tensors=\"pt\"\n            ).to(self.device)\n            \n            # Generate response\n            with torch.no_grad():\n                generated_ids = self.model.generate(\n                    **inputs,\n                    max_new_tokens=100,\n                    do_sample=True,\n                    temperature=0.7,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Decode response\n            response = self.processor.batch_decode(\n                generated_ids, \n                skip_special_tokens=True\n            )[0]\n            \n            # Extract generated text (remove input prompt)\n            if prompt in response:\n                response = response.replace(prompt, \"\").strip()\n            \n            return response\n            \n        except Exception as e:\n            print(f\"Error generating prediction: {e}\")\n            return \"Error generating prediction\"\n    \n    def evaluate_model(self, test_dataset, num_samples=10):\n        \"\"\"Evaluate model performance on test dataset\"\"\"\n        \n        results = []\n        \n        print(f\"Evaluating model on {min(num_samples, len(test_dataset))} samples...\")\n        \n        for i in tqdm(range(min(num_samples, len(test_dataset)))):\n            sample = test_dataset[i]\n            \n            if hasattr(sample, 'image_path'):\n                image_path = sample.image_path\n                original_prompt = sample.original_prompt\n            else:\n                # Handle dataset wrapper\n                idx = test_dataset.indices[i] if hasattr(test_dataset, 'indices') else i\n                data_item = test_dataset.dataset.data[idx] if hasattr(test_dataset, 'dataset') else test_dataset.data[idx]\n                image_path = data_item['image_path']\n                original_prompt = data_item['prompt']\n            \n            # Generate prediction\n            predicted = self.generate_prediction(image_path, \"Describe this image:\")\n            \n            results.append({\n                'image_path': image_path,\n                'actual': original_prompt,\n                'predicted': predicted,\n                'image_id': i\n            })\n        \n        return results\n    \n    def calculate_metrics(self, results):\n        \"\"\"Calculate evaluation metrics\"\"\"\n        bleu_scores = []\n        \n        for result in results:\n            actual = result['actual'].split()\n            predicted = result['predicted'].split()\n            \n            if len(predicted) > 0 and len(actual) > 0:\n                try:\n                    bleu = bleu_score.sentence_bleu([actual], predicted)\n                    bleu_scores.append(bleu)\n                except:\n                    bleu_scores.append(0.0)\n            else:\n                bleu_scores.append(0.0)\n        \n        avg_bleu = np.mean(bleu_scores)\n        \n        return {\n            'average_bleu': avg_bleu,\n            'bleu_scores': bleu_scores\n        }\n    \n    def plot_results(self, results, metrics, save_path=\"evaluation_results.png\"):\n        \"\"\"Plot evaluation results\"\"\"\n        \n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        fig.suptitle('Qwen VL Model Evaluation Results', fontsize=16, fontweight='bold')\n        \n        # Plot 1: Sample predictions with images\n        for i in range(min(4, len(results))):\n            row = i // 2\n            col = i % 2\n            \n            if row < 2 and col < 2:\n                try:\n                    img = Image.open(results[i]['image_path'])\n                    axes[row, col].imshow(img)\n                    axes[row, col].set_title(f\"Sample {i+1}\", fontweight='bold')\n                    axes[row, col].axis('off')\n                    \n                    # Add text below image\n                    actual_text = results[i]['actual'][:100] + \"...\" if len(results[i]['actual']) > 100 else results[i]['actual']\n                    pred_text = results[i]['predicted'][:100] + \"...\" if len(results[i]['predicted']) > 100 else results[i]['predicted']\n                    \n                    axes[row, col].text(0, -50, f\"Actual: {actual_text}\", \n                                      transform=axes[row, col].transAxes, \n                                      fontsize=8, color='green', weight='bold')\n                    axes[row, col].text(0, -80, f\"Predicted: {pred_text}\", \n                                      transform=axes[row, col].transAxes, \n                                      fontsize=8, color='blue')\n                except Exception as e:\n                    axes[row, col].text(0.5, 0.5, f\"Error loading image {i+1}\", \n                                      ha='center', va='center', transform=axes[row, col].transAxes)\n                    axes[row, col].axis('off')\n        \n        # Plot 5: BLEU Score Distribution\n        axes[1, 2].hist(metrics['bleu_scores'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n        axes[1, 2].set_title('BLEU Score Distribution')\n        axes[1, 2].set_xlabel('BLEU Score')\n        axes[1, 2].set_ylabel('Frequency')\n        axes[1, 2].axvline(metrics['average_bleu'], color='red', linestyle='--', \n                          label=f'Avg: {metrics[\"average_bleu\"]:.3f}')\n        axes[1, 2].legend()\n        \n        # Plot 6: Performance Summary\n        axes[0, 2].axis('off')\n        summary_text = f\"\"\"\n        Model Performance Summary\n        ========================\n        \n        Total Samples Evaluated: {len(results)}\n        Average BLEU Score: {metrics['average_bleu']:.4f}\n        \n        Best BLEU Score: {max(metrics['bleu_scores']):.4f}\n        Worst BLEU Score: {min(metrics['bleu_scores']):.4f}\n        \n        Median BLEU Score: {np.median(metrics['bleu_scores']):.4f}\n        Std Dev BLEU Score: {np.std(metrics['bleu_scores']):.4f}\n        \n        Model: {self.model_name}\n        Device: {self.device}\n        \"\"\"\n        \n        axes[0, 2].text(0.05, 0.95, summary_text, transform=axes[0, 2].transAxes, \n                       fontsize=10, verticalalignment='top', fontfamily='monospace',\n                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n        \n        plt.tight_layout()\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        print(f\"Results plot saved to {save_path}\")\n\n# Main execution function\ndef main():\n    \"\"\"Main training and evaluation pipeline\"\"\"\n    \n    # Configuration\n    DATA_DIR = \"./data\"  # Update this to your dataset directory\n    MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"\n    OUTPUT_DIR = \"./qwen_vl_finetuned\"\n    \n    # Initialize trainer\n    trainer = QwenVLTrainer(model_name=MODEL_NAME, data_dir=DATA_DIR)\n    \n    # Prepare datasets\n    train_dataset, val_dataset, test_dataset = trainer.prepare_datasets()\n    \n    # Train the model\n    print(\"Starting training process...\")\n    trained_model = trainer.train(\n        train_dataset, \n        val_dataset, \n        output_dir=OUTPUT_DIR,\n        num_epochs=3,\n        batch_size=2,  # Reduce if you have memory issues\n        learning_rate=2e-5\n    )\n    \n    # Evaluate the model\n    print(\"Evaluating model performance...\")\n    results = trainer.evaluate_model(test_dataset, num_samples=10)\n    \n    # Calculate metrics\n    metrics = trainer.calculate_metrics(results)\n    \n    # Print detailed results\n    print(\"\\n\" + \"=\"*50)\n    print(\"EVALUATION RESULTS\")\n    print(\"=\"*50)\n    \n    for i, result in enumerate(results):\n        print(f\"\\nSample {i+1}:\")\n        print(f\"Image: {os.path.basename(result['image_path'])}\")\n        print(f\"Actual: {result['actual']}\")\n        print(f\"Predicted: {result['predicted']}\")\n        print(\"-\" * 30)\n    \n    print(f\"\\nOverall Performance:\")\n    print(f\"Average BLEU Score: {metrics['average_bleu']:.4f}\")\n    \n    # Plot results\n    trainer.plot_results(results, metrics)\n    \n    return trainer, results, metrics\n\nif __name__ == \"__main__\":\n    # Print dataset info for Kaggle environment\n    print(\"\"\"\n    Kaggle Dataset Structure Expected:\n    \n    /kaggle/input/vllm-dataset/\n    ├── dataset.jsonl  # JSONL file with image-prompt pairs\n    └── images/\n        ├── aut-0003-00003809.jpg\n        └── ... (other images)\n    \n    JSONL Format:\n    Each line contains:\n    {\n      \"root\": {\n        \"image\": \"images/aut-0003-00003809.jpg\",\n        \"conversations\": [\n          {\"from\": \"human\", \"value\": \"<image>\"},\n          {\"from\": \"gpt\", \"value\": \"The image depicts...\"}\n        ]\n      }\n    }\n    \n    The code will automatically load your Kaggle dataset.\n    \"\"\")\n    \n    # Check if we're in Kaggle environment\n    import os\n    if os.path.exists('/kaggle/input'):\n        print(\"✓ Kaggle environment detected\")\n        print(\"✓ Using Kaggle dataset path: /kaggle/input/vllm-dataset\")\n    else:\n        print(\"⚠ Not in Kaggle environment - update DATA_DIR if needed\")\n    \n    # Run the training pipeline\n    trainer, results, metrics = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T12:49:07.606784Z","iopub.execute_input":"2025-09-11T12:49:07.607167Z","iopub.status.idle":"2025-09-11T12:49:53.513239Z","shell.execute_reply.started":"2025-09-11T12:49:07.607143Z","shell.execute_reply":"2025-09-11T12:49:53.511404Z"}},"outputs":[{"name":"stderr","text":"2025-09-11 12:49:29.763895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757594970.064686      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757594970.170350      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2370588636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mHFDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'bleu_score' from 'sklearn.metrics' (/usr/local/lib/python3.11/dist-packages/sklearn/metrics/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'bleu_score' from 'sklearn.metrics' (/usr/local/lib/python3.11/dist-packages/sklearn/metrics/__init__.py)","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}