{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12678181,"sourceType":"datasetVersion","datasetId":8011863}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## NanoDet ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cell 2: Setup NanoDet ---\n\n# Clone the repository\n!git clone https://github.com/RangiLyu/nanodet.git\n\n# Install requirements\n!pip install -q /kaggle/working/nanodet/\n\n# Change the current working directory to 'nanodet'\n# This is important so the script can find its files\n%cd /kaggle/working/nanodet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cell 3: Create Dataset Config ---\nimport os\n\n# Content for the apron_dataset.yml file\n# It uses the paths from your Kaggle environment\ndataset_yaml_content = \"\"\"\nclass_names: &class_names [\n    \"Safety Vehicle\", \"Business Aircraft\", \"Cargo Aircraft\", \"Common Aircraft\", \n    \"Container Trolley\", \"Conveyer Vehicle\", \"Helicopter\", \"Loading Ramp\", \n    \"Loading Vehicle\", \"Other Objects\", \"Other Vehicles\", \"Passenger Bus\", \n    \"Passenger Stairway\", \"Person\", \"Standard Car\", \"Tank Truck\", \n    \"Taxiing Vehicle\", \"Traffic Barrier\", \"Traffic Cone\", \"Traffic Cone Light\", \n    \"Traffic Sign\", \"Transport Container\", \"Transport Vehicle\"\n]\n\ntrain:\n  name: CocoDataset\n  img_path: /kaggle/input/airportvehicledataset/images\n  ann_path: /kaggle/working/coco_dataset/train.json\n  input_size: [416, 416]\n  keep_ratio: True\n  pipeline:\n    perspective: 0.0\n    scale: [0.5, 1.5]\n    stretch: [[1, 1], [1, 1]]\n    rotation: 0\n    shear: 0\n    translate: 0.2\n    flip: 0.5\n    brightness: 0.2\n    contrast: [0.8, 1.2]\n    saturation: [0.8, 1.2]\n    normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n\nval:\n  name: CocoDataset\n  img_path: /kaggle/input/airportvehicledataset/images\n  ann_path: /kaggle/working/coco_dataset/val.json\n  input_size: [416, 416]\n  keep_ratio: True\n  pipeline:\n    normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\n\"\"\"\n\n# Write the content to the correct location\nconfig_path = 'config/dataset/apron_dataset.yml'\nos.makedirs(os.path.dirname(config_path), exist_ok=True)\nwith open(config_path, 'w') as f:\n    f.write(dataset_yaml_content)\n\nprint(f\"✅ Created dataset config at: {config_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cell 4: Create Model Config ---\n\n# Full content for the nanodet-m-apron.yml file\n# This is a modified version of the default nanodet-m config\nmodel_yaml_content = \"\"\"\n# The directory to save logs and models\nsave_dir: /kaggle/working/workspace/nanodet_m_apron\n\n# Inherit from our custom dataset config\n_base_: ../dataset/apron_dataset.yml\n\nmodel:\n  arch:\n    name: NanoDetPlus\n    backbone:\n      name: ShuffleNetV2\n      model_size: 1.0x\n      out_stages: [2,3,4]\n      activation: LeakyReLU\n    fpn:\n      name: GhostPAN\n      in_channels: [116, 232, 464]\n      out_channels: 96\n      kernel_size: 5\n      num_extra_level: 1\n      use_depthwise: True\n      activation: LeakyReLU\n    head:\n      name: NanoDetPlusHead\n      num_classes: 23  # <--- CRITICAL: Changed from 80 to 23\n      input_channel: 96\n      feat_channels: 96\n      stacked_convs: 2\n      kernel_size: 5\n      strides: [8, 16, 32, 64]\n      conv_type: dws\n      activation: LeakyReLU\n      reg_max: 7\n    # Auxiliary head for NNDet-Plus\n    aux_head:\n      name: SimpleConvHead\n      num_classes: 23 # <--- CRITICAL: Changed from 80 to 23\n      input_channel: 192 # 96+96\n      feat_channels: 96\n      stacked_convs: 4\n      strides: [8, 16, 32, 64]\n      activation: LeakyReLU\n      reg_max: 7\n\nclass_names: *class_names\ndata:\n  train:\n    input_size: [416,416]\n  val:\n    input_size: [416,416]\n\ndevice:\n  batch_size_per_gpu: 16 # <-- Lower this (e.g., 8, 4) if you get a CUDA memory error\n  gpu_ids: [0]\n\nschedule:\n  resume: null\n  load_model: null\n  optimizer:\n    name: AdamW\n    lr: 0.001\n    weight_decay: 0.05\n  warmup:\n    name: linear\n    steps: 500\n    ratio: 0.0001\n  total_epochs: 120 # <-- You can change the number of epochs to train\n  lr_schedule:\n    name: CosineAnnealingLR\n    T_max: 120\n    eta_min: 0.00005\n  val_intervals: 10\n\nevaluator:\n  name: CocoDetectionEvaluator\n  save_key: mAP\n\nlog:\n  interval: 50\n\"\"\"\n\n# Write the content to the model config file\nwith open('config/nanodet-m-apron.yml', 'w') as f:\n    f.write(model_yaml_content)\n\nprint(\"✅ Created model config at: config/nanodet-m-apron.yml\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Final Training Cell (using PYTHONPATH) ---\nimport os\n\nprint(\"File structure is correct. Applying the PYTHONPATH fix.\")\n\n# The project root directory\nproject_root = '/kaggle/working/nanodet'\n\n# Change to the project directory (still good practice for relative paths in configs)\nos.chdir(project_root)\nprint(f\"Current Directory: {os.getcwd()}\")\n\n# --- The Command that Fixes the Issue ---\n# We set the PYTHONPATH environment variable for this specific command.\n# This tells the Python interpreter to add '/kaggle/working' to its\n# search path, allowing it to find the 'nanodet' module.\n!PYTHONPATH=/kaggle/working python tools/train.py -c config/nanodet-m-apron.yml","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Cell to modify train.py in-place ---\nimport os\n\n# The path to the script we need to modify\nscript_path = '/kaggle/working/nanodet/tools/train.py'\n\n# The lines of code to insert at the top of the script.\n# This forces the script to see the '/kaggle/working' directory.\ncode_to_insert = \"import sys\\nsys.path.insert(0, '/kaggle/working')\\n\\n\"\n\nprint(f\"Attempting to modify {script_path}...\")\n\ntry:\n    # Read the original content of the script\n    with open(script_path, 'r') as f:\n        original_content = f.read()\n\n    # Write the new content (our code + original code) back to the file\n    with open(script_path, 'w') as f:\n        f.write(code_to_insert + original_content)\n\n    print(\"✅ Script modified successfully.\")\n    \nexcept Exception as e:\n    print(f\"❌ An error occurred: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Final Run Cell ---\n\n# Change to the project directory\n%cd /kaggle/working/nanodet\n\n# Run the training script. It is now hard-coded to find the library.\n!python tools/train.py -c config/nanodet-m-apron.yml","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\nset -e\n\n# --- STEP 1: CLEANUP ---\necho \"--- Step 1: Cleaning up environment ---\"\npip uninstall -y nanodet || echo \"nanodet was not installed, which is good.\"\nrm -rf /kaggle/nanodet\necho \"Cleanup complete.\"\n\n# --- STEP 2: CLONE & ISOLATED INSTALL ---\necho -e \"\\n--- Step 2: Cloning and Creating an Isolated Installation ---\"\ngit clone https://github.com/RangiLyu/nanodet.git /kaggle/nanodet\ncd /kaggle/nanodet\n\n# --- THE ISOLATION FIX ---\n# We use --ignore-installed to force pip to install fresh packages and\n# ignore the conflicting ones pre-installed on Kaggle.\n\necho \"Installing dependencies into an isolated user environment...\"\n\n# We don't need to sanitize the requirements file anymore, we just need to install them.\n# The --ignore-installed flag is the key.\npip install --ignore-installed -r requirements.txt\n\n# Manually install a compatible mmcv-full, also ignoring system packages\npip install --ignore-installed \"mmcv-full<2.0.0,>=1.7.0\" -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\n\n# Now, build and install nanodet itself.\n# --no-deps tells it not to re-verify dependencies we just installed.\necho \"Building and installing nanodet...\"\npip install --ignore-installed --no-deps .\necho \"✅ NanoDet package and all dependencies installed successfully in isolation.\"\n\n# --- STEP 3: CREATE CONFIG FILES ---\necho -e \"\\n--- Step 3: Creating configuration files ---\"\nmkdir -p config/dataset\n\ncat <<'EOF' > config/dataset/apron_dataset.yml\nclass_names: &class_names [\n    \"Safety Vehicle\", \"Business Aircraft\", \"Cargo Aircraft\", \"Common Aircraft\", \n    \"Container Trolley\", \"Conveyer Vehicle\", \"Helicopter\", \"Loading Ramp\", \n    \"Loading Vehicle\", \"Other Objects\", \"Other Vehicles\", \"Passenger Bus\", \n    \"Passenger Stairway\", \"Person\", \"Standard Car\", \"Tank Truck\", \n    \"Taxiing Vehicle\", \"Traffic Barrier\", \"Traffic Cone\", \"Traffic Cone Light\", \n    \"Traffic Sign\", \"Transport Container\", \"Transport Vehicle\"\n]\ntrain:\n  name: CocoDataset\n  img_path: /kaggle/input/airportvehicledataset/images\n  ann_path: /kaggle/coco_dataset/train.json\n  input_size: [416, 416]\n  keep_ratio: True\n  pipeline:\n    perspective: 0.0\n    scale: [0.5, 1.5]\n    stretch: [[1, 1], [1, 1]]\n    rotation: 0\n    shear: 0\n    translate: 0.2\n    flip: 0.5\n    brightness: 0.2\n    contrast: [0.8, 1.2]\n    saturation: [0.8, 1.2]\n    normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\nval:\n  name: CocoDataset\n  img_path: /kaggle/input/airportvehicledataset/images\n  ann_path: /kaggle/coco_dataset/val.json\n  input_size: [416, 416]\n  keep_ratio: True\n  pipeline:\n    normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\nEOF\necho \"Created apron_dataset.yml\"\n\ncat <<'EOF' > config/nanodet-m-apron.yml\nsave_dir: /kaggle/workspace/nanodet_m_apron\n_base_: ../dataset/apron_dataset.yml\nmodel:\n  arch:\n    name: NanoDetPlus\n    backbone:\n      name: ShuffleNetV2\n      model_size: 1.0x\n      out_stages: [2,3,4]\n      activation: LeakyReLU\n    fpn:\n      name: GhostPAN\n      in_channels: [116, 232, 464]\n      out_channels: 96\n      kernel_size: 5\n      num_extra_level: 1\n      use_depthwise: True\n      activation: LeakyReLU\n    head:\n      name: NanoDetPlusHead\n      num_classes: 23\n      input_channel: 96\n      feat_channels: 96\n      stacked_convs: 2\n      kernel_size: 5\n      strides: [8, 16, 32, 64]\n      conv_type: dws\n      activation: LeakyReLU\n      reg_max: 7\n    aux_head:\n      name: SimpleConvHead\n      num_classes: 23\n      input_channel: 192\n      feat_channels: 96\n      stacked_convs: 4\n      strides: [8, 16, 32, 64]\n      activation: LeakyReLU\n      reg_max: 7\nclass_names: *class_names\ndata:\n  train:\n    input_size: [416,416]\n  val:\n    input_size: [416,416]\ndevice:\n  batch_size_per_gpu: 8\n  gpu_ids: [0]\nschedule:\n  resume: null\n  load_model: null\n  optimizer:\n    name: AdamW\n    lr: 0.001\n    weight_decay: 0.05\n  warmup:\n    name: linear\n    steps: 500\n    ratio: 0.0001\n  total_epochs: 120\n  lr_schedule:\n    name: CosineAnnealingLR\n    T_max: 120\n    eta_min: 0.00005\n  val_intervals: 10\nevaluator:\n  name: CocoDetectionEvaluator\n  save_key: mAP\nlog:\n  interval: 50\nEOF\necho \"Created nanodet-m-apron.yml\"\n\n# --- STEP 4: TRAIN ---\necho -e \"\\n--- Step 4: Starting the training ---\"\npython tools/train.py -c config/nanodet-m-apron.yml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T13:32:57.309889Z","iopub.execute_input":"2025-08-06T13:32:57.310184Z","iopub.status.idle":"2025-08-06T13:44:53.561520Z","shell.execute_reply.started":"2025-08-06T13:32:57.310162Z","shell.execute_reply":"2025-08-06T13:44:53.560397Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Cleaning up environment ---\nCleanup complete.\n\n--- Step 2: Cloning and Creating an Isolated Installation ---\nInstalling dependencies into an isolated user environment...\nCollecting Cython (from -r requirements.txt (line 1))\n  Using cached cython-3.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\nCollecting imagesize (from -r requirements.txt (line 2))\n  Using cached imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting matplotlib (from -r requirements.txt (line 3))\n  Using cached matplotlib-3.10.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nCollecting numpy (from -r requirements.txt (line 4))\n  Using cached numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\nCollecting omegaconf>=2.0.1 (from -r requirements.txt (line 5))\n  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting onnx (from -r requirements.txt (line 6))\n  Using cached onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting onnx-simplifier (from -r requirements.txt (line 7))\n  Using cached onnx_simplifier-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nCollecting opencv-python (from -r requirements.txt (line 8))\n  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\nCollecting pyaml (from -r requirements.txt (line 9))\n  Using cached pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\nCollecting pycocotools (from -r requirements.txt (line 10))\n  Using cached pycocotools-2.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nCollecting pytorch-lightning<2.0.0,>=1.9.0 (from -r requirements.txt (line 11))\n  Using cached pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\nCollecting tabulate (from -r requirements.txt (line 12))\n  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting termcolor (from -r requirements.txt (line 14))\n  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\nCollecting torch<2.0,>=1.10 (from -r requirements.txt (line 15))\n  Using cached torch-1.13.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\nCollecting torchmetrics (from -r requirements.txt (line 16))\n  Using cached torchmetrics-1.8.0-py3-none-any.whl.metadata (21 kB)\nCollecting torchvision (from -r requirements.txt (line 17))\n  Using cached torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nCollecting tqdm (from -r requirements.txt (line 18))\n  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nCollecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 3))\n  Using cached contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\nCollecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 3))\n  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\nCollecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 3))\n  Using cached fonttools-4.59.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (107 kB)\nCollecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 3))\n  Using cached kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\nCollecting packaging>=20.0 (from matplotlib->-r requirements.txt (line 3))\n  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting pillow>=8 (from matplotlib->-r requirements.txt (line 3))\n  Using cached pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\nCollecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 3))\n  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\nCollecting python-dateutil>=2.7 (from matplotlib->-r requirements.txt (line 3))\n  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0.1->-r requirements.txt (line 5))\n  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting PyYAML>=5.1.0 (from omegaconf>=2.0.1->-r requirements.txt (line 5))\n  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\nCollecting protobuf>=4.25.1 (from onnx->-r requirements.txt (line 6))\n  Using cached protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\nCollecting typing_extensions>=4.7.1 (from onnx->-r requirements.txt (line 6))\n  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting rich (from onnx-simplifier->-r requirements.txt (line 7))\n  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\nCollecting numpy (from -r requirements.txt (line 4))\n  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\nCollecting fsspec>2021.06.0 (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>=1.9.0->-r requirements.txt (line 11))\n  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\nCollecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<2.0.0,>=1.9.0->-r requirements.txt (line 11))\n  Using cached lightning_utilities-0.15.1-py3-none-any.whl.metadata (5.7 kB)\nCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 13))\n  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\nCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 13))\n  Using cached grpcio-1.74.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 13))\n  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\nCollecting setuptools>=41.0.0 (from tensorboard->-r requirements.txt (line 13))\n  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\nCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 13))\n  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\nCollecting werkzeug>=1.0.1 (from tensorboard->-r requirements.txt (line 13))\n  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.0,>=1.10->-r requirements.txt (line 15))\n  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.0,>=1.10->-r requirements.txt (line 15))\n  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.0,>=1.10->-r requirements.txt (line 15))\n  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.0,>=1.10->-r requirements.txt (line 15))\n  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting wheel (from nvidia-cublas-cu11==11.10.3.66->torch<2.0,>=1.10->-r requirements.txt (line 15))\n  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\nINFO: pip is looking at multiple versions of torchmetrics to determine which version is compatible with other requirements. This could take a while.\nCollecting torchmetrics (from -r requirements.txt (line 16))\n  Using cached torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.7.2-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.7.0-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.6.3-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-1.6.2-py3-none-any.whl.metadata (20 kB)\nINFO: pip is still looking at multiple versions of torchmetrics to determine which version is compatible with other requirements. This could take a while.\n  Using cached torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from -r requirements.txt (line 17))\n  Using cached torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n  Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n  Using cached torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n  Using cached torchvision-0.20.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n  Using cached torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n  Using cached torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n  Using cached torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\nINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n  Using cached torchvision-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Using cached torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Using cached torchvision-0.17.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Using cached torchvision-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\nCollecting requests (from torchvision->-r requirements.txt (line 17))\n  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\nCollecting torchvision (from -r requirements.txt (line 17))\n  Using cached torchvision-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Using cached torchvision-0.16.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Using cached torchvision-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n  Using cached torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\n  Using cached torchvision-0.15.1-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\nCollecting torchmetrics (from -r requirements.txt (line 16))\n  Using cached torchmetrics-1.5.1-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-1.5.0-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-1.4.3-py3-none-any.whl.metadata (19 kB)\n  Using cached torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n  Using cached torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n  Using cached torchmetrics-1.4.0-py3-none-any.whl.metadata (19 kB)\nCollecting pretty-errors==1.2.25 (from torchmetrics->-r requirements.txt (line 16))\n  Using cached pretty_errors-1.2.25-py3-none-any.whl.metadata (12 kB)\nCollecting colorama (from pretty-errors==1.2.25->torchmetrics->-r requirements.txt (line 16))\n  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\nCollecting torchmetrics (from -r requirements.txt (line 16))\n  Using cached torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n  Using cached torchmetrics-1.3.1-py3-none-any.whl.metadata (19 kB)\n  Using cached torchmetrics-1.3.0.post0-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-1.2.0-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.1.2-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.1.1-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.1.0-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.0.3-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.0.2-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.0.1-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-1.0.0-py3-none-any.whl.metadata (21 kB)\n  Using cached torchmetrics-0.11.4-py3-none-any.whl.metadata (15 kB)\n  Using cached torchmetrics-0.11.3-py3-none-any.whl.metadata (15 kB)\n  Using cached torchmetrics-0.11.2-py3-none-any.whl.metadata (15 kB)\n  Using cached torchmetrics-0.11.1-py3-none-any.whl.metadata (16 kB)\n  Using cached torchmetrics-0.11.0-py3-none-any.whl.metadata (16 kB)\n  Using cached torchmetrics-0.10.3-py3-none-any.whl.metadata (15 kB)\n  Using cached torchmetrics-0.10.2-py3-none-any.whl.metadata (15 kB)\n  Using cached torchmetrics-0.10.1-py3-none-any.whl.metadata (15 kB)\n  Using cached torchmetrics-0.10.0-py3-none-any.whl.metadata (15 kB)\n  Using cached torchmetrics-0.9.3-py3-none-any.whl.metadata (17 kB)\n  Using cached torchmetrics-0.9.2-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-0.9.1-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-0.9.0-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-0.8.2-py3-none-any.whl.metadata (20 kB)\nCollecting pyDeprecate==0.3.* (from torchmetrics->-r requirements.txt (line 16))\n  Using cached pyDeprecate-0.3.2-py3-none-any.whl.metadata (10 kB)\nCollecting torchmetrics (from -r requirements.txt (line 16))\n  Using cached torchmetrics-0.8.1-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-0.8.0-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-0.7.3-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-0.7.2-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-0.7.1-py3-none-any.whl.metadata (20 kB)\n  Using cached torchmetrics-0.7.0-py3-none-any.whl.metadata (20 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting six>1.9 (from tensorboard->-r requirements.txt (line 13))\n  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n  Using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n  Using cached tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting protobuf>=4.25.1 (from onnx->-r requirements.txt (line 6))\n  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n  Using cached tensorboard-2.16.1-py3-none-any.whl.metadata (1.6 kB)\nCollecting tf-keras>=2.15.0 (from tensorboard->-r requirements.txt (line 13))\n  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.16.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting tf-keras-nightly (from tensorboard->-r requirements.txt (line 13))\n  Using cached tf_keras_nightly-2.20.0.dev2025062209-py3-none-any.whl.metadata (1.9 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\nCollecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 13))\n  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\nCollecting google-auth-oauthlib<2,>=0.5 (from tensorboard->-r requirements.txt (line 13))\n  Using cached google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\nINFO: pip is looking at multiple versions of tensorboard to determine which version is compatible with other requirements. This could take a while.\n  Using cached tensorboard-2.15.0-py3-none-any.whl.metadata (1.7 kB)\n  Using cached tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\nCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard->-r requirements.txt (line 13))\n  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n  Using cached tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\nINFO: pip is still looking at multiple versions of tensorboard to determine which version is compatible with other requirements. This could take a while.\n  Using cached tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n  Using cached tensorboard-2.12.2-py3-none-any.whl.metadata (1.8 kB)\nCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard->-r requirements.txt (line 13))\n  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.12.1-py3-none-any.whl.metadata (1.8 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Using cached tensorboard-2.12.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard->-r requirements.txt (line 13))\n  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.11.0-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.10.0-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.9.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard->-r requirements.txt (line 13))\n  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.7.0-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.6.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting google-auth<2,>=1.6.3 (from tensorboard->-r requirements.txt (line 13))\n  Using cached google_auth-1.35.0-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-2.5.0-py3-none-any.whl.metadata (2.0 kB)\n  Using cached tensorboard-2.4.1-py3-none-any.whl.metadata (2.0 kB)\n  Using cached tensorboard-2.4.0-py3-none-any.whl.metadata (2.0 kB)\n  Using cached tensorboard-2.3.0-py3-none-any.whl.metadata (2.0 kB)\n  Using cached tensorboard-2.2.2-py3-none-any.whl.metadata (2.0 kB)\n  Using cached tensorboard-2.2.1-py3-none-any.whl.metadata (2.0 kB)\n  Using cached tensorboard-2.2.0-py3-none-any.whl.metadata (2.0 kB)\n  Using cached tensorboard-2.1.1-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.1.0-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.0.2-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.0.1-py3-none-any.whl.metadata (1.9 kB)\n  Using cached tensorboard-2.0.0-py3-none-any.whl.metadata (1.8 kB)\n  Using cached tensorboard-1.15.0-py3-none-any.whl.metadata (1.8 kB)\n  Using cached tensorboard-1.14.0-py3-none-any.whl.metadata (1.8 kB)\n  Using cached tensorboard-1.13.1-py3-none-any.whl.metadata (1.7 kB)\n  Using cached tensorboard-1.13.0-py3-none-any.whl.metadata (1.7 kB)\n  Using cached tensorboard-1.12.2-py3-none-any.whl.metadata (1.7 kB)\n  Using cached tensorboard-1.12.1-py3-none-any.whl.metadata (1.7 kB)\n  Using cached tensorboard-1.12.0-py3-none-any.whl.metadata (1.7 kB)\n  Using cached tensorboard-1.11.0-py3-none-any.whl.metadata (1.7 kB)\n  Using cached tensorboard-1.10.0-py3-none-any.whl.metadata (1.7 kB)\n  Using cached tensorboard-1.9.0-py3-none-any.whl.metadata (1.7 kB)\n  Using cached tensorboard-1.8.0-py3-none-any.whl.metadata (1.7 kB)\nCollecting html5lib==0.9999999 (from tensorboard->-r requirements.txt (line 13))\n  Using cached html5lib-0.9999999.tar.gz (889 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting bleach==1.5.0 (from tensorboard->-r requirements.txt (line 13))\n  Using cached bleach-1.5.0-py2.py3-none-any.whl.metadata (8.9 kB)\nCollecting tensorboard (from -r requirements.txt (line 13))\n  Using cached tensorboard-1.7.0-py3-none-any.whl.metadata (1.7 kB)\n  Using cached tensorboard-1.6.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting onnx-simplifier (from -r requirements.txt (line 7))\n  Using cached onnx_simplifier-0.4.35-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nCollecting pytorch-lightning<2.0.0,>=1.9.0 (from -r requirements.txt (line 11))\n  Using cached pytorch_lightning-1.9.4-py3-none-any.whl.metadata (22 kB)\n  Using cached pytorch_lightning-1.9.3-py3-none-any.whl.metadata (22 kB)\n  Using cached pytorch_lightning-1.9.2-py3-none-any.whl.metadata (22 kB)\n  Using cached pytorch_lightning-1.9.1-py3-none-any.whl.metadata (22 kB)\n  Using cached pytorch_lightning-1.9.0-py3-none-any.whl.metadata (23 kB)\nCollecting pycocotools (from -r requirements.txt (line 10))\n  Using cached pycocotools-2.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n  Using cached pycocotools-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n  Using cached pycocotools-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\n  Downloading pycocotools-2.0.5.tar.gz (24 kB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\n  Downloading pycocotools-2.0.4.tar.gz (106 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.6/106.6 kB 2.8 MB/s eta 0:00:00\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\n  Downloading pycocotools-2.0.3.tar.gz (106 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.5/106.5 kB 4.2 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\n  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting opencv-python (from -r requirements.txt (line 8))\n  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nCollecting numpy (from -r requirements.txt (line 4))\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 2.6 MB/s eta 0:00:00\n\nThe conflict is caused by:\n    The user requested torch<2.0 and >=1.10\n    pytorch-lightning 1.9.0 depends on torch>=1.10.0\n    torchmetrics 1.8.0 depends on torch>=2.0.0\n    The user requested torch<2.0 and >=1.10\n    pytorch-lightning 1.9.0 depends on torch>=1.10.0\n    torchmetrics 1.7.4 depends on torch>=2.0.0\n    The user requested torch<2.0 and >=1.10\n    pytorch-lightning 1.9.0 depends on torch>=1.10.0\n    torchmetrics 1.7.3 depends on torch>=2.0.0\n    The user requested torch<2.0 and >=1.10\n    pytorch-lightning 1.9.0 depends on torch>=1.10.0\n    torchmetrics 1.7.2 depends on torch>=2.0.0\n    The user requested torch<2.0 and >=1.10\n    pytorch-lightning 1.9.0 depends on torch>=1.10.0\n    torchmetrics 1.7.1 depends on torch>=2.0.0\n    The user requested torch<2.0 and >=1.10\n    pytorch-lightning 1.9.0 depends on torch>=1.10.0\n    torchmetrics 1.7.0 depends on torch>=2.0.0\n    The user requested torch<2.0 and >=1.10\n    pytorch-lightning 1.9.0 depends on torch>=1.10.0\n    torchmetrics 1.6.3 depends on torch>=2.0.0\n    The user requested torch<2.0 and >=1.10\n    pytorch-lightning 1.9.0 depends on torch>=1.10.0\n    torchmetrics 1.6.2 depends on torch>=2.0.0\n    The user requested torch<2.0 and >=1.10\n    pytorch-lightning 1.9.0 depends on torch>=1.10.0\n    torchmetrics 1.6.1 depends on torch>=2.0.0\n    The user requested torch<2.0 and >=1.10\n    pytorch-lightning 1.9.0 depends on torch>=1.10.0\n    torchmetrics 1.6.0 depends on torch>=2.0.0\n\nTo fix this you could try to:\n1. loosen the range of package versions you've specified\n2. remove package versions to allow pip to attempt to solve the dependency conflict\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Skipping nanodet as it is not installed.\nCloning into '/kaggle/nanodet'...\nWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/pydeprecate/\nERROR: Cannot install -r requirements.txt (line 11), -r requirements.txt (line 16) and torch<2.0 and >=1.10 because these package versions have conflicting dependencies.\nERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_207/877824057.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set -e\\n\\n# --- STEP 1: CLEANUP ---\\necho \"--- Step 1: Cleaning up environment ---\"\\npip uninstall -y nanodet || echo \"nanodet was not installed, which is good.\"\\nrm -rf /kaggle/nanodet\\necho \"Cleanup complete.\"\\n\\n# --- STEP 2: CLONE & ISOLATED INSTALL ---\\necho -e \"\\\\n--- Step 2: Cloning and Creating an Isolated Installation ---\"\\ngit clone https://github.com/RangiLyu/nanodet.git /kaggle/nanodet\\ncd /kaggle/nanodet\\n\\n# --- THE ISOLATION FIX ---\\n# We use --ignore-installed to force pip to install fresh packages and\\n# ignore the conflicting ones pre-installed on Kaggle.\\n\\necho \"Installing dependencies into an isolated user environment...\"\\n\\n# We don\\'t need to sanitize the requirements file anymore, we just need to install them.\\n# The --ignore-installed flag is the key.\\npip install --ignore-installed -r requirements.txt\\n\\n# Manually install a compatible mmcv-full, also ignoring system packages\\npip install --ignore-installed \"mmcv-full<2.0.0,>=1.7.0\" -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\\n\\n# Now, build and install nanodet itself.\\n# --no-deps tells it not to re-verify dependencies we just installed.\\necho \"Building and installing nanodet...\"\\npip install --ignore-installed --no-deps .\\necho \"✅ NanoDet package and all dependencies installed successfully in isolation.\"\\n\\n# --- STEP 3: CREATE CONFIG FILES ---\\necho -e \"\\\\n--- Step 3: Creating configuration files ---\"\\nmkdir -p config/dataset\\n\\ncat <<\\'EOF\\' > config/dataset/apron_dataset.yml\\nclass_names: &class_names [\\n    \"Safety Vehicle\", \"Business Aircraft\", \"Cargo Aircraft\", \"Common Aircraft\", \\n    \"Container Trolley\", \"Conveyer Vehicle\", \"Helicopter\", \"Loading Ramp\", \\n    \"Loading Vehicle\", \"Other Objects\", \"Other Vehicles\", \"Passenger Bus\", \\n    \"Passenger Stairway\", \"Person\", \"Standard Car\", \"Tank Truck\", \\n    \"Taxiing Vehicle\", \"Traffic Barrier\", \"Traffic Cone\", \"Traffic Cone Light\", \\n    \"Traffic Sign\", \"Transport Container\", \"Transport Vehicle\"\\n]\\ntrain:\\n  name: CocoDataset\\n  img_path: /kaggle/input/airportvehicledataset/images\\n  ann_path: /kaggle/coco_dataset/train.json\\n  input_size: [416, 416]\\n  keep_ratio: True\\n  pipeline:\\n    perspective: 0.0\\n    scale: [0.5, 1.5]\\n    stretch: [[1, 1], [1, 1]]\\n    rotation: 0\\n    shear: 0\\n    translate: 0.2\\n    flip: 0.5\\n    brightness: 0.2\\n    contrast: [0.8, 1.2]\\n    saturation: [0.8, 1.2]\\n    normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\\nval:\\n  name: CocoDataset\\n  img_path: /kaggle/input/airportvehicledataset/images\\n  ann_path: /kaggle/coco_dataset/val.json\\n  input_size: [416, 416]\\n  keep_ratio: True\\n  pipeline:\\n    normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\\nEOF\\necho \"Created apron_dataset.yml\"\\n\\ncat <<\\'EOF\\' > config/nanodet-m-apron.yml\\nsave_dir: /kaggle/workspace/nanodet_m_apron\\n_base_: ../dataset/apron_dataset.yml\\nmodel:\\n  arch:\\n    name: NanoDetPlus\\n    backbone:\\n      name: ShuffleNetV2\\n      model_size: 1.0x\\n      out_stages: [2,3,4]\\n      activation: LeakyReLU\\n    fpn:\\n      name: GhostPAN\\n      in_channels: [116, 232, 464]\\n      out_channels: 96\\n      kernel_size: 5\\n      num_extra_level: 1\\n      use_depthwise: True\\n      activation: LeakyReLU\\n    head:\\n      name: NanoDetPlusHead\\n      num_classes: 23\\n      input_channel: 96\\n      feat_channels: 96\\n      stacked_convs: 2\\n      kernel_size: 5\\n      strides: [8, 16, 32, 64]\\n      conv_type: dws\\n      activation: LeakyReLU\\n      reg_max: 7\\n    aux_head:\\n      name: SimpleConvHead\\n      num_classes: 23\\n      input_channel: 192\\n      feat_channels: 96\\n      stacked_convs: 4\\n      strides: [8, 16, 32, 64]\\n      activation: LeakyReLU\\n      reg_max: 7\\nclass_names: *class_names\\ndata:\\n  train:\\n    input_size: [416,416]\\n  val:\\n    input_size: [416,416]\\ndevice:\\n  batch_size_per_gpu: 8\\n  gpu_ids: [0]\\nschedule:\\n  resume: null\\n  load_model: null\\n  optimizer:\\n    name: AdamW\\n    lr: 0.001\\n    weight_decay: 0.05\\n  warmup:\\n    name: linear\\n    steps: 500\\n    ratio: 0.0001\\n  total_epochs: 120\\n  lr_schedule:\\n    name: CosineAnnealingLR\\n    T_max: 120\\n    eta_min: 0.00005\\n  val_intervals: 10\\nevaluator:\\n  name: CocoDetectionEvaluator\\n  save_key: mAP\\nlog:\\n  interval: 50\\nEOF\\necho \"Created nanodet-m-apron.yml\"\\n\\n# --- STEP 4: TRAIN ---\\necho -e \"\\\\n--- Step 4: Starting the training ---\"\\npython tools/train.py -c config/nanodet-m-apron.yml\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'set -e\\n\\n# --- STEP 1: CLEANUP ---\\necho \"--- Step 1: Cleaning up environment ---\"\\npip uninstall -y nanodet || echo \"nanodet was not installed, which is good.\"\\nrm -rf /kaggle/nanodet\\necho \"Cleanup complete.\"\\n\\n# --- STEP 2: CLONE & ISOLATED INSTALL ---\\necho -e \"\\\\n--- Step 2: Cloning and Creating an Isolated Installation ---\"\\ngit clone https://github.com/RangiLyu/nanodet.git /kaggle/nanodet\\ncd /kaggle/nanodet\\n\\n# --- THE ISOLATION FIX ---\\n# We use --ignore-installed to force pip to install fresh packages and\\n# ignore the conflicting ones pre-installed on Kaggle.\\n\\necho \"Installing dependencies into an isolated user environment...\"\\n\\n# We don\\'t need to sanitize the requirements file anymore, we just need to install them.\\n# The --ignore-installed flag is the key.\\npip install --ignore-installed -r requirements.txt\\n\\n# Manually install a compatible mmcv-full, also ignoring system packages\\npip install --ignore-installed \"mmcv-full<2.0.0,>=1.7.0\" -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\\n\\n# Now, build and install nanodet itself.\\n# --no-deps tells it not to re-verify dependencies we just installed.\\necho \"Building and installing nanodet...\"\\npip install --ignore-installed --no-deps .\\necho \"\\xe2\\x9c\\x85 NanoDet package and all dependencies installed successfully in isolation.\"\\n\\n# --- STEP 3: CREATE CONFIG FILES ---\\necho -e \"\\\\n--- Step 3: Creating configuration files ---\"\\nmkdir -p config/dataset\\n\\ncat <<\\'EOF\\' > config/dataset/apron_dataset.yml\\nclass_names: &class_names [\\n    \"Safety Vehicle\", \"Business Aircraft\", \"Cargo Aircraft\", \"Common Aircraft\", \\n    \"Container Trolley\", \"Conveyer Vehicle\", \"Helicopter\", \"Loading Ramp\", \\n    \"Loading Vehicle\", \"Other Objects\", \"Other Vehicles\", \"Passenger Bus\", \\n    \"Passenger Stairway\", \"Person\", \"Standard Car\", \"Tank Truck\", \\n    \"Taxiing Vehicle\", \"Traffic Barrier\", \"Traffic Cone\", \"Traffic Cone Light\", \\n    \"Traffic Sign\", \"Transport Container\", \"Transport Vehicle\"\\n]\\ntrain:\\n  name: CocoDataset\\n  img_path: /kaggle/input/airportvehicledataset/images\\n  ann_path: /kaggle/coco_dataset/train.json\\n  input_size: [416, 416]\\n  keep_ratio: True\\n  pipeline:\\n    perspective: 0.0\\n    scale: [0.5, 1.5]\\n    stretch: [[1, 1], [1, 1]]\\n    rotation: 0\\n    shear: 0\\n    translate: 0.2\\n    flip: 0.5\\n    brightness: 0.2\\n    contrast: [0.8, 1.2]\\n    saturation: [0.8, 1.2]\\n    normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\\nval:\\n  name: CocoDataset\\n  img_path: /kaggle/input/airportvehicledataset/images\\n  ann_path: /kaggle/coco_dataset/val.json\\n  input_size: [416, 416]\\n  keep_ratio: True\\n  pipeline:\\n    normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\\nEOF\\necho \"Created apron_dataset.yml\"\\n\\ncat <<\\'EOF\\' > config/nanodet-m-apron.yml\\nsave_dir: /kaggle/workspace/nanodet_m_apron\\n_base_: ../dataset/apron_dataset.yml\\nmodel:\\n  arch:\\n    name: NanoDetPlus\\n    backbone:\\n      name: ShuffleNetV2\\n      model_size: 1.0x\\n      out_stages: [2,3,4]\\n      activation: LeakyReLU\\n    fpn:\\n      name: GhostPAN\\n      in_channels: [116, 232, 464]\\n      out_channels: 96\\n      kernel_size: 5\\n      num_extra_level: 1\\n      use_depthwise: True\\n      activation: LeakyReLU\\n    head:\\n      name: NanoDetPlusHead\\n      num_classes: 23\\n      input_channel: 96\\n      feat_channels: 96\\n      stacked_convs: 2\\n      kernel_size: 5\\n      strides: [8, 16, 32, 64]\\n      conv_type: dws\\n      activation: LeakyReLU\\n      reg_max: 7\\n    aux_head:\\n      name: SimpleConvHead\\n      num_classes: 23\\n      input_channel: 192\\n      feat_channels: 96\\n      stacked_convs: 4\\n      strides: [8, 16, 32, 64]\\n      activation: LeakyReLU\\n      reg_max: 7\\nclass_names: *class_names\\ndata:\\n  train:\\n    input_size: [416,416]\\n  val:\\n    input_size: [416,416]\\ndevice:\\n  batch_size_per_gpu: 8\\n  gpu_ids: [0]\\nschedule:\\n  resume: null\\n  load_model: null\\n  optimizer:\\n    name: AdamW\\n    lr: 0.001\\n    weight_decay: 0.05\\n  warmup:\\n    name: linear\\n    steps: 500\\n    ratio: 0.0001\\n  total_epochs: 120\\n  lr_schedule:\\n    name: CosineAnnealingLR\\n    T_max: 120\\n    eta_min: 0.00005\\n  val_intervals: 10\\nevaluator:\\n  name: CocoDetectionEvaluator\\n  save_key: mAP\\nlog:\\n  interval: 50\\nEOF\\necho \"Created nanodet-m-apron.yml\"\\n\\n# --- STEP 4: TRAIN ---\\necho -e \"\\\\n--- Step 4: Starting the training ---\"\\npython tools/train.py -c config/nanodet-m-apron.yml\\n'' returned non-zero exit status 1."],"ename":"CalledProcessError","evalue":"Command 'b'set -e\\n\\n# --- STEP 1: CLEANUP ---\\necho \"--- Step 1: Cleaning up environment ---\"\\npip uninstall -y nanodet || echo \"nanodet was not installed, which is good.\"\\nrm -rf /kaggle/nanodet\\necho \"Cleanup complete.\"\\n\\n# --- STEP 2: CLONE & ISOLATED INSTALL ---\\necho -e \"\\\\n--- Step 2: Cloning and Creating an Isolated Installation ---\"\\ngit clone https://github.com/RangiLyu/nanodet.git /kaggle/nanodet\\ncd /kaggle/nanodet\\n\\n# --- THE ISOLATION FIX ---\\n# We use --ignore-installed to force pip to install fresh packages and\\n# ignore the conflicting ones pre-installed on Kaggle.\\n\\necho \"Installing dependencies into an isolated user environment...\"\\n\\n# We don\\'t need to sanitize the requirements file anymore, we just need to install them.\\n# The --ignore-installed flag is the key.\\npip install --ignore-installed -r requirements.txt\\n\\n# Manually install a compatible mmcv-full, also ignoring system packages\\npip install --ignore-installed \"mmcv-full<2.0.0,>=1.7.0\" -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/index.html\\n\\n# Now, build and install nanodet itself.\\n# --no-deps tells it not to re-verify dependencies we just installed.\\necho \"Building and installing nanodet...\"\\npip install --ignore-installed --no-deps .\\necho \"\\xe2\\x9c\\x85 NanoDet package and all dependencies installed successfully in isolation.\"\\n\\n# --- STEP 3: CREATE CONFIG FILES ---\\necho -e \"\\\\n--- Step 3: Creating configuration files ---\"\\nmkdir -p config/dataset\\n\\ncat <<\\'EOF\\' > config/dataset/apron_dataset.yml\\nclass_names: &class_names [\\n    \"Safety Vehicle\", \"Business Aircraft\", \"Cargo Aircraft\", \"Common Aircraft\", \\n    \"Container Trolley\", \"Conveyer Vehicle\", \"Helicopter\", \"Loading Ramp\", \\n    \"Loading Vehicle\", \"Other Objects\", \"Other Vehicles\", \"Passenger Bus\", \\n    \"Passenger Stairway\", \"Person\", \"Standard Car\", \"Tank Truck\", \\n    \"Taxiing Vehicle\", \"Traffic Barrier\", \"Traffic Cone\", \"Traffic Cone Light\", \\n    \"Traffic Sign\", \"Transport Container\", \"Transport Vehicle\"\\n]\\ntrain:\\n  name: CocoDataset\\n  img_path: /kaggle/input/airportvehicledataset/images\\n  ann_path: /kaggle/coco_dataset/train.json\\n  input_size: [416, 416]\\n  keep_ratio: True\\n  pipeline:\\n    perspective: 0.0\\n    scale: [0.5, 1.5]\\n    stretch: [[1, 1], [1, 1]]\\n    rotation: 0\\n    shear: 0\\n    translate: 0.2\\n    flip: 0.5\\n    brightness: 0.2\\n    contrast: [0.8, 1.2]\\n    saturation: [0.8, 1.2]\\n    normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\\nval:\\n  name: CocoDataset\\n  img_path: /kaggle/input/airportvehicledataset/images\\n  ann_path: /kaggle/coco_dataset/val.json\\n  input_size: [416, 416]\\n  keep_ratio: True\\n  pipeline:\\n    normalize: [[103.53, 116.28, 123.675], [57.375, 57.12, 58.395]]\\nEOF\\necho \"Created apron_dataset.yml\"\\n\\ncat <<\\'EOF\\' > config/nanodet-m-apron.yml\\nsave_dir: /kaggle/workspace/nanodet_m_apron\\n_base_: ../dataset/apron_dataset.yml\\nmodel:\\n  arch:\\n    name: NanoDetPlus\\n    backbone:\\n      name: ShuffleNetV2\\n      model_size: 1.0x\\n      out_stages: [2,3,4]\\n      activation: LeakyReLU\\n    fpn:\\n      name: GhostPAN\\n      in_channels: [116, 232, 464]\\n      out_channels: 96\\n      kernel_size: 5\\n      num_extra_level: 1\\n      use_depthwise: True\\n      activation: LeakyReLU\\n    head:\\n      name: NanoDetPlusHead\\n      num_classes: 23\\n      input_channel: 96\\n      feat_channels: 96\\n      stacked_convs: 2\\n      kernel_size: 5\\n      strides: [8, 16, 32, 64]\\n      conv_type: dws\\n      activation: LeakyReLU\\n      reg_max: 7\\n    aux_head:\\n      name: SimpleConvHead\\n      num_classes: 23\\n      input_channel: 192\\n      feat_channels: 96\\n      stacked_convs: 4\\n      strides: [8, 16, 32, 64]\\n      activation: LeakyReLU\\n      reg_max: 7\\nclass_names: *class_names\\ndata:\\n  train:\\n    input_size: [416,416]\\n  val:\\n    input_size: [416,416]\\ndevice:\\n  batch_size_per_gpu: 8\\n  gpu_ids: [0]\\nschedule:\\n  resume: null\\n  load_model: null\\n  optimizer:\\n    name: AdamW\\n    lr: 0.001\\n    weight_decay: 0.05\\n  warmup:\\n    name: linear\\n    steps: 500\\n    ratio: 0.0001\\n  total_epochs: 120\\n  lr_schedule:\\n    name: CosineAnnealingLR\\n    T_max: 120\\n    eta_min: 0.00005\\n  val_intervals: 10\\nevaluator:\\n  name: CocoDetectionEvaluator\\n  save_key: mAP\\nlog:\\n  interval: 50\\nEOF\\necho \"Created nanodet-m-apron.yml\"\\n\\n# --- STEP 4: TRAIN ---\\necho -e \"\\\\n--- Step 4: Starting the training ---\"\\npython tools/train.py -c config/nanodet-m-apron.yml\\n'' returned non-zero exit status 1.","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}