{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12842556,"sourceType":"datasetVersion","datasetId":8122458}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch transformers einops datasets bitsandbytes accelerate pandas pillow peft","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Full script for fine-tuning Moondream2 on a custom Parquet dataset\n# This is adapted from the official Moondream fine-tuning notebook:\n# https://github.com/vikhyat/moondream/blob/main/notebooks/Finetuning.ipynb\n# Requirements: Run on a machine with GPU, install dependencies via:\n# pip install torch transformers einops datasets bitsandbytes accelerate pandas pillow peft\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,get_scheduler\nfrom torch.optim import AdamW\nfrom datasets import load_dataset  # Not used here, but for reference\nfrom tqdm import tqdm\nimport pandas as pd\nfrom io import BytesIO\nfrom PIL import Image\nfrom peft import LoraConfig, get_peft_model  # For efficient fine-tuning with LoRA\n\n# Custom Dataset for Parquet file\nclass ParquetDataset(Dataset):\n    def __init__(self, parquet_path):\n        self.df = pd.read_parquet(parquet_path)\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(BytesIO(row['image'])).convert(\"RGB\")\n        question = \"Describe this image.\"  # Adjust if captions are prompts, e.g., \"What is the prompt for this image?\"\n        answer = row['caption']\n        return {\n            \"image\": image,\n            \"qa\": [\n                {\n                    \"question\": question,\n                    \"answer\": answer,\n                }\n            ]\n        }\n\n# Collate function for batching\ndef collate_fn(batch):\n    images = [item['image'] for item in batch]\n    texts = [f\"USER: <image>{item['qa'][0]['question']}\\nASSISTANT: {item['qa'][0]['answer']}</s>\" for item in batch]\n    return images, texts\n\n# Load model and tokenizer\nmodel_id = \"vikhyatk/moondream2\"\nrevision = \"2024-05-20\"  # Use latest stable\ntokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    revision=revision,\n    torch_dtype=torch.float16,  # Use bfloat16 if on Ampere+ GPU\n    device_map=\"auto\"  # Maps to GPU\n)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n# Optional: Use LoRA for parameter-efficient fine-tuning to save VRAM\nlora_config = LoraConfig(\n    r=16,  # Rank\n    lora_alpha=32,\n    target_modules=[\"qkv\",        # vision attention projection\n                   \"proj\",       # vision output projection\n                   \"out_proj\"],  # Adjust based on model architecture\n    lora_dropout=0.05,\n    bias=\"none\"\n)\nmodel = get_peft_model(model, lora_config)\n\n# Prepare dataset and dataloader\n# Assume you have train.parquet; split if needed\ndataset = ParquetDataset(\"/kaggle/input/appron-prompt-dataset/dataset.parquet\")\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)  # Adjust batch_size based on VRAM\n\n# Optimizer and scheduler\noptimizer = AdamW(model.parameters(), lr=1e-5)\nnum_epochs = 1  # Increase as needed\nnum_training_steps = num_epochs * len(dataloader)\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)\n\n# Training loop\nmodel.train()\nfor epoch in range(num_epochs):\n    for images, texts in tqdm(dataloader):\n        # Tokenize texts\n        encodings = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n        \n        # Encode images\n        image_embeds = model.vision_encoder(images)  # Assuming model has vision_encoder; check model docs if needed\n        \n        # Forward pass (adapt if model uses different input format)\n        outputs = model(\n            input_ids=encodings.input_ids,\n            attention_mask=encodings.attention_mask,\n            image_embeds=image_embeds,\n            labels=encodings.input_ids  # For causal LM loss\n        )\n        \n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n    \n    print(f\"Epoch {epoch+1} completed.\")\n\n# Save fine-tuned model\nmodel.save_pretrained(\"kaggle/working/finetuned_moondream\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import inspect\nprint(inspect.signature(model.forward))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Testing before and after fine-tuning\n# Note: For before, reload the original model without LoRA\n\n# Before fine-tuning (use original model)\noriginal_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    revision=revision,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\ntest_image = Image.open(\"path/to/your_test_image.jpg\").convert(\"RGB\")\nenc_image = original_model.encode_image(test_image)\nprint(\"Before fine-tuning:\")\nprint(original_model.answer_question(enc_image, \"Describe this image.\", tokenizer))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After fine-tuning\nfinetuned_model = AutoModelForCausalLM.from_pretrained(\n    \"path/to/save/finetuned_moondream\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\nenc_image = finetuned_model.encode_image(test_image)\nprint(\"After fine-tuning:\")\nprint(finetuned_model.answer_question(enc_image, \"Describe this image.\", tokenizer))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install torch transformers einops datasets pandas pillow peft accelerate bitsandbytes\n\nimport os\nfrom io import BytesIO\nfrom PIL import Image\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\n\n# ------------------\n# 1) Dataset (Parquet)\n# ------------------\nclass ParquetDataset(torch.utils.data.Dataset):\n    def __init__(self, parquet_path, image_col_guess=(\"image_bytes\",\"images\",\"image\",\"image_path\"), caption_col_guess=(\"caption\",\"text\",\"answer\",\"labels\")):\n        self.df = pd.read_parquet(parquet_path)\n        # auto-detect columns\n        self.img_col = next((c for c in image_col_guess if c in self.df.columns), None)\n        self.cap_col = next((c for c in caption_col_guess if c in self.df.columns), None)\n        if self.img_col is None:\n            raise ValueError(f\"Could not find an image column among {image_col_guess}. Have columns: {list(self.df.columns)}\")\n        if self.cap_col is None:\n            raise ValueError(f\"Could not find a caption/answer column among {caption_col_guess}. Have columns: {list(self.df.columns)}\")\n\n    def __len__(self):\n        return len(self.df)\n\n    def _load_image(self, row):\n        val = row[self.img_col]\n        # If it's file path\n        if isinstance(val, str):\n            return Image.open(val).convert(\"RGB\")\n        # If it's bytes (Parquet as bytes)\n        if isinstance(val, (bytes, bytearray, memoryview)):\n            return Image.open(BytesIO(val)).convert(\"RGB\")\n        # If it’s already PIL Image (rare)\n        if isinstance(val, Image.Image):\n            return val.convert(\"RGB\")\n        raise TypeError(f\"Unsupported image type in column '{self.img_col}': {type(val)}\")\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = self._load_image(row)\n        answer = str(row[self.cap_col])\n        # simple instruction format (Moondream will read image from its methods)\n        question = \"Describe this image.\"\n        # We’ll train only on assistant answer tokens (mask prompt during loss)\n        text = f\"USER: <image>{question}\\nASSISTANT: {answer}</s>\"\n        return image, text\n\ndef collate_fn(batch):\n    images, texts = zip(*batch)\n    return list(images), list(texts)\n\n# ------------------\n# 2) Load Moondream2\n# ------------------\nmodel_id = \"vikhyatk/moondream2\"\n# Use a recent revision for best compatibility & bugfixes in custom code:\nrevision = \"2025-01-09\"  # or omit to get latest; docs show this date\n# IMPORTANT: Do NOT create AutoProcessor for Moondream. Use the model methods.\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    revision=revision,\n    trust_remote_code=True,\n    torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n    device_map=\"auto\",\n)\n\n# Text tokenizer (no processor!)\ntokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision, trust_remote_code=True)\n# Moondream’s tokenizer often lacks a pad token; fix the padding error you saw:\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# ------------------\n# 3) LoRA (correct target module for Moondream)\n# ------------------\nlora_cfg = LoraConfig(\n    r=8,               # tune to your VRAM\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"qkv\",        # vision attention projection\n                   \"proj\",       # vision output projection\n                   \"out_proj\"],   # <- Moondream-specific\n)\nmodel = get_peft_model(model, lora_cfg)\n\n# Optional: gradient checkpointing on the text transformer\nif hasattr(model, \"text_model\") and hasattr(model.text_model, \"transformer\"):\n    try:\n        model.text_model.transformer.gradient_checkpointing_enable()\n    except Exception:\n        pass\n\n# ------------------\n# 4) Data\n# ------------------\nparquet_path = \"/kaggle/input/appron-prompt-dataset/dataset.parquet\"  # change if needed\ndataset = ParquetDataset(parquet_path)\nloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)\n\n# ------------------\n# 5) Optimizer & training utils\n# ------------------\nfrom torch.optim import AdamW\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\ndef mask_user_tokens(input_ids, labels, user_prefix=\"USER:\"):\n    # Simple label masking: ignore loss for the prompt part if you wish.\n    # Here we keep it simple and train on full sequence. If you want strict SFT,\n    # parse and set labels for USER tokens to -100.\n    return labels\n\nmodel.train()\n\n# ------------------\n# 6) Training loop using Moondream’s own image preprocessing\n#    We avoid passing 'images' into a tokenizer/processor.\n# ------------------\ndevice = next(iter(model.parameters())).device\n\nfor epoch in range(1):  # increase as needed\n    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n    for images, texts in pbar:\n        # 1) Encode images with Moondream’s built-in preprocessing\n        #    (You can pre-encode per image to reuse across steps if wanted.)\n        encoded_images = [model.encode_image(img) for img in images]\n\n        # 2) Tokenize text\n        tok = tokenizer(\n            texts,\n            padding=True,\n            truncation=True,\n            max_length=1024,\n            return_tensors=\"pt\",\n        )\n        input_ids = tok.input_ids.to(device)\n        attention_mask = tok.attention_mask.to(device)\n\n        # 3) Build labels (optionally mask the prompt tokens)\n        labels = input_ids.clone()\n        labels = mask_user_tokens(input_ids, labels)\n\n        # 4) Forward (use Moondream custom forward that accepts images via kwargs)\n        #    Moondream’s HF wrapper routes 'images=...' correctly.\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            images=encoded_images,     # <- pass encoded images here\n            labels=labels,\n        )\n        loss = outputs.loss\n\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n# ------------------\n# 7) Save\n# ------------------\nsave_dir = \"./finetuned_moondream\"\nos.makedirs(save_dir, exist_ok=True)\nmodel.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\n\nprint(\"Saved to\", save_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get update && apt-get install -y libvips -q\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install torch transformers pandas pillow peft accelerate bitsandbytes\n\nimport os\nfrom io import BytesIO\nfrom PIL import Image\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\n\n# 1) Dataset\nclass ParquetDataset(Dataset):\n    def __init__(self, path, img_cols=(\"image_bytes\",\"images\",\"image\",\"image_path\"), cap_cols=(\"caption\",\"text\")):\n        df = pd.read_parquet(path)\n        self.img_col = next((c for c in img_cols if c in df.columns), None)\n        self.cap_col = next((c for c in cap_cols if c in df.columns), None)\n        if not self.img_col or not self.cap_col:\n            raise ValueError(f\"Missing columns. Found: {df.columns.tolist()}\")\n        self.df = df\n\n    def __len__(self): return len(self.df)\n    def __getitem__(self, i):\n        row = self.df.iloc[i]\n        val = row[self.img_col]\n        img = Image.open(BytesIO(val) if isinstance(val, (bytes, bytearray)) else val).convert(\"RGB\")\n        ans = str(row[self.cap_col])\n        txt = f\"USER: <image>Describe this image.\\nASSISTANT: {ans}</s>\"\n        return img, txt\n\ndef collate_fn(batch):\n    imgs, txts = zip(*batch)\n    return list(imgs), list(txts)\n\n# 2) Model + Tokenizer\nmodel_id = \"vikhyatk/moondream2\"\nrevision = \"2025-06-21\"  # latest with full compatibility\nmodel = AutoModelForCausalLM.from_pretrained(model_id, revision=revision, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# 3) Device logic (critical fix) — ensures all parts of the model live on same device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(f\"Using device: {device}\")\nmodel = model.to(device)  # mandatory fix :contentReference[oaicite:1]{index=1}\n\n# 4) LoRA\nlora_cfg = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05,\n                     bias=\"none\", task_type=\"CAUSAL_LM\",\n                     target_modules=[\"qkv\",\"proj\",\"out_proj\"])\nmodel = get_peft_model(model, lora_cfg)\n\n# 5) DataLoader setup\npath = \"/kaggle/input/appron-prompt-dataset/dataset.parquet\"\ndataset = ParquetDataset(path)\nloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n\n# 6) Optimizer\nfrom torch.optim import AdamW\nopt = AdamW(model.parameters(), lr=1e-5)\n\nmodel.train()\nfor epoch in range(1):\n    for images, texts in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n        enc_imgs = []\n        for img in images:\n            enc = model.encode_image(img)  # all internals now on correct device\n            # enc.kv_cache = enc.kv_cache.to(device)\n            enc_imgs.append(enc)\n\n        tok = tokenizer(texts, padding=True, truncation=True,\n                        return_tensors=\"pt\").to(device)\n        labels = tok.input_ids.clone()\n\n        outs = model(input_ids=tok.input_ids,\n                     attention_mask=tok.attention_mask,\n                     images=enc_imgs,\n                     labels=labels)\n        loss = outs.loss\n\n        opt.zero_grad(set_to_none=True)\n        loss.backward()\n        opt.step()\n\n        tqdm.write(f\"Loss: {loss.item():.4f}\")\n\n# 7) Saving\nodir = \"./finetuned_moondream\"\nos.makedirs(odir, exist_ok=True)\nmodel.save_pretrained(odir)\ntokenizer.save_pretrained(odir)\nprint(\"Saved to\", odir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import inspect\nreal_model = model.get_base_model() if hasattr(model, \"get_base_model\") else model\nprint(real_model.__class__)\nprint(inspect.signature(real_model.forward))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------\n# 0) Install prerequisites\n# -------------------------\n# pip install torch transformers peft einops pandas pillow tqdm accelerate bitsandbytes\n\nimport os\nfrom io import BytesIO\nfrom PIL import Image\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\n\n# -------------------------\n# 1) Dataset\n# -------------------------\nclass ParquetDataset(Dataset):\n    def __init__(self, parquet_path, image_col_guess=(\"image_bytes\",\"images\",\"image\",\"image_path\"),\n                 caption_col_guess=(\"caption\",\"text\",\"answer\",\"labels\")):\n        self.df = pd.read_parquet(parquet_path)\n        self.img_col = next((c for c in image_col_guess if c in self.df.columns), None)\n        self.cap_col = next((c for c in caption_col_guess if c in self.df.columns), None)\n        if self.img_col is None or self.cap_col is None:\n            raise ValueError(\"Cannot find image or caption column\")\n\n    def __len__(self):\n        return len(self.df)\n\n    def _load_image(self, row):\n        val = row[self.img_col]\n        if isinstance(val, str):\n            return Image.open(val).convert(\"RGB\")\n        if isinstance(val, (bytes, bytearray, memoryview)):\n            return Image.open(BytesIO(val)).convert(\"RGB\")\n        if isinstance(val, Image.Image):\n            return val.convert(\"RGB\")\n        raise TypeError(f\"Unsupported image type: {type(val)}\")\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = self._load_image(row)\n        answer = str(row[self.cap_col])\n        question = \"Describe this image.\"\n        text = f\"USER: <image>{question}\\nASSISTANT: {answer}</s>\"\n        return image, text\n\ndef collate_fn(batch):\n    images, texts = zip(*batch)\n    return list(images), list(texts)\n\n# -------------------------\n# 2) Load Moondream2 + tokenizer\n# -------------------------\nmodel_id = \"vikhyatk/moondream2\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    torch_dtype=torch.float16 if torch.cuda.is_available() else None,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndevice = next(model.parameters()).device\n\n# -------------------------\n# 3) Apply LoRA\n# -------------------------\nlora_cfg = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"qkv\", \"proj\", \"out_proj\"],\n)\nmodel = get_peft_model(model, lora_cfg)\n\n# -------------------------\n# 4) Dataset / DataLoader\n# -------------------------\nparquet_path = \"/kaggle/input/appron-prompt-dataset/dataset.parquet\"\ndataset = ParquetDataset(parquet_path)\nloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)\n\n# -------------------------\n# 5) Optimizer\n# -------------------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nmodel.train()\n\n# -------------------------\n# 6) Training Loop\n# -------------------------\nfor epoch in range(1):\n    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n    for images, texts in pbar:\n        # 1) Encode images (ensure correct device)\n        encoded_images = [model.encode_image(img) for img in images]\n        for e in encoded_images:\n            # Moondream EncodedImage may contain kv_cache on CPU internally\n            if hasattr(e, \"kv_cache\") and e.kv_cache is not None:\n                e.kv_cache = e.kv_cache.to(device)\n\n        # 2) Tokenize text\n        tok = tokenizer(texts, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\")\n        input_ids = tok.input_ids.to(device)\n        attention_mask = tok.attention_mask.to(device)\n        labels = input_ids.clone()\n\n        # 3) Call Moondream forward using internal methods\n        # Forward only accepts **kwargs for internal usage\n        # images should be passed via `images=encoded_images`\n        forward_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask,\n                          \"images\": encoded_images, \"labels\": labels}\n\n        # Peft-wrapped model: must use **kwargs forwarding\n        outputs = model(**forward_kwargs)\n        loss = outputs.loss\n\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n# -------------------------\n# 7) Save\n# -------------------------\nsave_dir = \"./finetuned_moondream\"\nos.makedirs(save_dir, exist_ok=True)\nmodel.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\nprint(\"Saved to\", save_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T16:11:12.893645Z","iopub.execute_input":"2025-09-05T16:11:12.893944Z","iopub.status.idle":"2025-09-05T16:11:22.433361Z","shell.execute_reply.started":"2025-09-05T16:11:12.893913Z","shell.execute_reply":"2025-09-05T16:11:22.432293Z"}},"outputs":[{"name":"stderr","text":"Epoch 1:   0%|          | 0/150 [00:03<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1597/3514165797.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# 1) Encode images (ensure correct device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mencoded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# Moondream EncodedImage may contain kv_cache on CPU internally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_1597/3514165797.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# 1) Encode images (ensure correct device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mencoded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoded_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# Moondream EncodedImage may contain kv_cache on CPU internally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/28a4beffe5d08b8104e1dbc10b5cd29a5a13722c/moondream.py\u001b[0m in \u001b[0;36mencode_image\u001b[0;34m(self, image, settings)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mpos_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         return EncodedImage(\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/28a4beffe5d08b8104e1dbc10b5cd29a5a13722c/moondream.py\u001b[0m in \u001b[0;36m_prefill\u001b[0;34m(self, x, attn_mask, pos_ids, lora)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mlora\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     ):\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     def _decode_one_tok(\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/28a4beffe5d08b8104e1dbc10b5cd29a5a13722c/text.py\u001b[0m in \u001b[0;36mtext_decoder\u001b[0;34m(x, w, attn_mask, position_ids, config, lora)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0ml_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         l_attn = attn(\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0ml_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/28a4beffe5d08b8104e1dbc10b5cd29a5a13722c/text.py\u001b[0m in \u001b[0;36mattn\u001b[0;34m(x, w, freqs_cis, kv_cache, attn_mask, n_heads, n_kv_heads, position_ids, lora)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kv_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs_cis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs_cis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kv_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/28a4beffe5d08b8104e1dbc10b5cd29a5a13722c/rope.py\u001b[0m in \u001b[0;36mapply_rotary_emb\u001b[0;34m(x, freqs_cis, position_ids, num_heads, rot_dim, interleave)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Complex multiplication: (a + bi) * (c + di) = (ac - bd) + (ad + bc)i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mxq_out_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxq_r\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfreqs_cos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mxq_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfreqs_sin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mxq_out_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxq_r\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfreqs_sin\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mxq_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfreqs_cos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mxq_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxq_out_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxq_out_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!","output_type":"error"}],"execution_count":2}]}