{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12842556,"sourceType":"datasetVersion","datasetId":8122458}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers==4.43.3 accelerate datasets peft bitsandbytes pillow evaluate trl==0.9.4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install \"trl==0.9.4\" -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset, Image\n\n# Load parquet file\nparquet_path = \"/kaggle/input/appron-prompt-dataset/dataset.parquet\"\ndf = pd.read_parquet(parquet_path)\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_pandas(df)\n\n# Cast the 'image' column as Image feature\ndataset = dataset.cast_column(\"image\", Image(decode=True))\n\n# First split: train/test\ndataset = dataset.train_test_split(test_size=0.1)\ntrain_dataset = dataset[\"train\"]\ntest_dataset = dataset[\"test\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_data(sample):\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are an assistant that describes images.\"},\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"image\", \"image\": sample[\"image\"]}],\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": [{\"type\": \"text\", \"text\": sample[\"caption\"]}],\n            },\n        ]\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = [format_data(sample) for sample in train_dataset]\ntest_dataset = [format_data(sample) for sample in test_dataset]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoProcessor\n\nmodel = \"Intel/llava-gemma-2b\"\nprocessor = AutoProcessor.from_pretrained(model)\n\ndef preprocess(sample):\n    proc = processor.apply_chat_template(\n        sample[\"messages\"],\n        tokenize=True,\n        add_generation_prompt=False,\n        return_tensors=\"pt\"\n    )\n    return {k: v[0] for k, v in proc.items()}\n\n# train_dataset = train_dataset.map(preprocess, remove_columns=train_dataset.column_names)\n# test_dataset = test_dataset.map(preprocess, remove_columns=test_dataset.column_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./llava-gemma-finetuned\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    learning_rate=2e-4,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    evaluation_strategy=\"steps\",\n    eval_steps=200,\n    save_steps=500,\n    save_total_limit=2,\n    bf16=True,\n    report_to=\"none\",   # no wandb\n    push_to_hub=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\nimport math\n\nbleu = evaluate.load(\"bleu\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    # Compute perplexity from loss\n    loss = logits.mean()  # trainer already logs loss\n    perplexity = math.exp(loss) if loss < 20 else float(\"inf\")\n    \n    # Decode predictions & labels\n    predictions = processor.tokenizer.batch_decode(\n        logits.argmax(-1), skip_special_tokens=True\n    )\n    references = processor.tokenizer.batch_decode(\n        labels, skip_special_tokens=True\n    )\n    \n    bleu_score = bleu.compute(predictions=predictions, references=[[r] for r in references])\n    return {\"perplexity\": perplexity, \"bleu\": bleu_score[\"bleu\"]}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForVision2Seq, TrainingArguments\nfrom trl import SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=processor.tokenizer,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    packing=False\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers, trl\nprint(\"Transformers:\", transformers.__version__)\nprint(\"TRL:\", trl.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Convert logs to DataFrame\nlogs = pd.DataFrame(trainer.state.log_history)\n\n# Training loss\ntrain_loss = logs[logs[\"loss\"].notnull()]\nplt.plot(train_loss[\"step\"], train_loss[\"loss\"], label=\"Training Loss\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Curve\")\nplt.legend()\nplt.show()\n\n# Evaluation metrics\nif \"eval_loss\" in logs.columns:\n    eval_logs = logs[logs[\"eval_loss\"].notnull()]\n    plt.plot(eval_logs[\"step\"], eval_logs[\"eval_loss\"], label=\"Eval Loss\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Eval Loss\")\n    plt.title(\"Evaluation Loss Curve\")\n    plt.legend()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torchvision import transforms\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM, get_scheduler\nimport numpy as np\n\n# --------------------------\n# CONFIG\n# --------------------------\nMODEL_ID = \"Intel/llava-gemma-2b\"\nBATCH_SIZE = 2\nNUM_EPOCHS = 2\nLR = 2e-5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMAX_LENGTH = 1024\n\n# --------------------------\n# LOAD MODEL + PROCESSOR\n# --------------------------\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID).to(DEVICE)\n\n# --------------------------\n# DUMMY DATASET (replace with your JSON/CSV)\n# Each sample has: {\"image\": <path>, \"messages\": [{\"role\":\"user\",\"content\":\"...\"} , {\"role\":\"assistant\",\"content\":\"...\"}]}\n# --------------------------\nclass MyDataset(Dataset):\n    def __init__(self, samples):\n        self.samples = samples\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]\n\n# --------------------------\n# COLLATE FN\n# --------------------------\nclass Collator:\n    def __init__(self, processor, max_length=1024):\n        self.processor = processor\n        self.max_length = max_length\n\n    def __call__(self, batch):\n        input_ids_list, attn_list, pixel_values_list, labels_list = [], [], [], []\n\n        for sample in batch:\n            # Load image\n            img = sample[\"image\"]\n            if isinstance(img, str):   # path\n                img = Image.open(img).convert(\"RGB\")\n\n            # Apply chat template\n            messages = sample[\"messages\"]\n            proc = self.processor.apply_chat_template(\n                messages,\n                tokenize=True,\n                add_generation_prompt=False,\n                return_tensors=\"pt\"\n            )\n\n            input_ids = proc[\"input_ids\"][0]\n            attention_mask = proc[\"attention_mask\"][0]\n\n            # Get image features\n            img_out = self.processor(images=img, return_tensors=\"pt\")\n            pixel_values = img_out[\"pixel_values\"][0]\n\n            # Labels = copy input_ids but mask user/system tokens (-100)\n            labels = input_ids.clone()\n            # Simple masking rule: only keep assistant tokens\n            if messages[-1][\"role\"] == \"assistant\":\n                assistant_text = messages[-1][\"content\"]\n                with_assistant = self.processor.apply_chat_template(\n                    [{\"role\": \"assistant\", \"content\": assistant_text}],\n                    tokenize=True,\n                    add_generation_prompt=False,\n                    return_tensors=\"pt\"\n                )\n                keep_len = with_assistant[\"input_ids\"].shape[1]\n                labels[:-keep_len] = -100\n            else:\n                labels[:] = -100\n\n            # Truncate\n            input_ids = input_ids[:self.max_length]\n            attention_mask = attention_mask[:self.max_length]\n            labels = labels[:self.max_length]\n\n            input_ids_list.append(input_ids)\n            attn_list.append(attention_mask)\n            labels_list.append(labels)\n            pixel_values_list.append(pixel_values)\n\n        # Pad text\n        input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n            input_ids_list, batch_first=True, padding_value=processor.tokenizer.pad_token_id\n        )\n        attn_padded = torch.nn.utils.rnn.pad_sequence(\n            attn_list, batch_first=True, padding_value=0\n        )\n        labels_padded = torch.nn.utils.rnn.pad_sequence(\n            labels_list, batch_first=True, padding_value=-100\n        )\n\n        pixel_values = torch.stack(pixel_values_list)  # (B, C, H, W)\n\n        return {\n            \"input_ids\": input_ids_padded,\n            \"attention_mask\": attn_padded,\n            \"labels\": labels_padded,\n            \"pixel_values\": pixel_values,\n        }\n\n# --------------------------\n# TRAINING LOOP\n# --------------------------\ndef train_fn(train_dataset):\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        collate_fn=Collator(processor, max_length=MAX_LENGTH),\n    )\n\n    optimizer = AdamW(model.parameters(), lr=LR)\n    num_training_steps = NUM_EPOCHS * len(train_loader)\n    lr_scheduler = get_scheduler(\n        \"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps\n    )\n\n    model.train()\n    for epoch in range(NUM_EPOCHS):\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n        for step, batch in enumerate(pbar):\n            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n            pbar.set_postfix({\"loss\": loss.item()})\n\n    torch.save(model.state_dict(), \"llava_gemma2b_finetuned.pt\")\n\n# --------------------------\n# EXAMPLE USAGE\n# --------------------------\nsamples = [\n    {\n        \"image\": \"your_image1.jpg\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"What is in this picture?\"},\n            {\"role\": \"assistant\", \"content\": \"A cat sitting on a sofa.\"},\n        ],\n    },\n    {\n        \"image\": \"your_image2.jpg\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Describe the object.\"},\n            {\"role\": \"assistant\", \"content\": \"A red car on the street.\"},\n        ],\n    },\n]\n\ndataset = MyDataset(samples)\ntrain_fn(dataset)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch tensorboard pillow transformers datasets accelerate evaluate bitsandbytes trl peft","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers==4.43.3 accelerate datasets peft bitsandbytes pillow evaluate trl==0.9.4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine-Tuning LLaVA-Gemma-2B on an Image-Caption Dataset using Hugging Face TRL and PEFT\n# Updated to fix RuntimeError: only Tensors of floating point dtype can require gradients\n# by dequantizing the multi_modal_projector before applying PEFT\n\n# Step 1: Install required libraries\n# Run in your environment:\n# pip install torch tensorboard pillow transformers datasets accelerate evaluate bitsandbytes trl peft\n\nimport torch\nfrom huggingface_hub import login\nfrom datasets import load_dataset\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom transformers import BitsAndBytesConfig, LlavaForConditionalGeneration, AutoProcessor, TrainingArguments\nfrom trl import SFTTrainer\n\n# Step 2: Log in to Hugging Face (if pushing model)\n# login(token=\"your_hf_token_here\")  # Replace with your Hugging Face token\n\n# Step 3: Define the model ID\nmodel_id = \"Intel/llava-gemma-2b\"\n\n# Step 4: Load the processor\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n# Step 5: Prepare the dataset\n# Assume you have an image-caption dataset in Hugging Face format, e.g., \"laion/coco\" or your custom one.\n# For demonstration, we'll use a small subset. Replace with your dataset.\nimport pandas as pd\nfrom datasets import Dataset, Image\n\n# Load parquet file\nparquet_path = \"/kaggle/input/appron-prompt-dataset/dataset.parquet\"\ndf = pd.read_parquet(parquet_path)\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_pandas(df)\n\n# Cast the 'image' column as Image feature\ndataset = dataset.cast_column(\"image\", Image(decode=True))\n\n# Format the dataset into conversational format for TRL\nsystem_message = \"You are a helpful image captioning assistant.\"\nuser_prompt = \"Describe the image in detail.\"\n\ndef format_example(sample):\n    return {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"image\", \"image\": sample[\"image\"]},\n                {\"type\": \"text\", \"text\": user_prompt}\n            ]},\n            {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": dataset['caption']}]}\n        ]\n    }\n\nformatted_dataset = [format_example(sample) for sample in dataset]\n\n# Step 6: Load the model with 4-bit quantization for efficiency\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# Step 6.5: Dequantize the multi_modal_projector to avoid gradient error\n# Since multi_modal_projector will be in modules_to_save, its parameters need to be float dtype\nfrom bitsandbytes.functional import dequantize_4bit\n\ndef dequantize_module(module):\n    if not hasattr(module, 'weight'):  # Skip if not a linear layer with weight\n        return module\n    quant_state = module.weight.quant_state\n    dequant_weight = dequantize_4bit(module.weight.data, quant_state).to(torch.bfloat16)\n    bias = module.bias\n    # Get device from parameters\n    device = next(module.parameters()).device\n    new_module = torch.nn.Linear(\n        module.in_features,\n        module.out_features,\n        bias=bias is not None,\n        dtype=dequant_weight.dtype,\n        device=device\n    )\n    new_module.weight.data = dequant_weight\n    if bias is not None:\n        new_module.bias.data = bias\n    return new_module\n\n# Apply to both linear layers in the projector\nmodel.multi_modal_projector.linear_1 = dequantize_module(model.multi_modal_projector.linear_1)\nmodel.multi_modal_projector.linear_2 = dequantize_module(model.multi_modal_projector.linear_2)\n\n# Step 7: Prepare for LoRA fine-tuning\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Target LLM modules\n    modules_to_save=[\"multi_modal_projector\"]  # Save the vision-language connector\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\n\n# Step 8: Set up training arguments using TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./llava-gemma-2b-finetuned\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,  # Adjust based on GPU memory (Gemma-2B is small)\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    max_grad_norm=1.0,\n    optim=\"paged_adamw_8bit\",\n    fp16=True,\n    remove_unused_columns=False,\n    report_to=\"tensorboard\",\n    save_steps=500,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"loss\",\n    greater_is_better=False,\n    push_to_hub=False  # Set to True to push to HF\n)\n\n# Step 9: Initialize the SFTTrainer, passing SFT-specific args here\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=formatted_dataset,\n    peft_config=lora_config,  # Already applied, but for reference\n    tokenizer=processor.tokenizer,  # Use the tokenizer from processor\n    dataset_text_field=\"messages\",\n    packing=False,  # No packing for VLM\n    max_seq_length=512  # Adjust as needed\n)\n\n# Step 10: Start fine-tuning\ntrainer.train()\n\n# Step 11: Save the fine-tuned model\ntrainer.save_model(\"./llava-gemma-2b-finetuned\")\nprocessor.save_pretrained(\"./llava-gemma-2b-finetuned\")\n\n# Explanation of Fix:\n# The TypeError occurs because in older versions of TRL (e.g., <= v0.8.x), SFTConfig does not support parameters like max_seq_length, dataset_text_field, or packing.\n# These are SFTTrainer-specific arguments.\n# By switching to transformers.TrainingArguments for the core training params and passing the SFT-specific args directly to SFTTrainer, the code becomes compatible with both old and new TRL versions.\n# If you update TRL to the latest version (pip install -U trl), you co","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T11:29:54.616230Z","iopub.execute_input":"2025-08-26T11:29:54.616555Z","iopub.status.idle":"2025-08-26T11:30:33.008348Z","shell.execute_reply.started":"2025-08-26T11:29:54.616530Z","shell.execute_reply":"2025-08-26T11:30:33.007255Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c064b1d9d5ed4386addef7a2893d274d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:2007: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_414/2426226027.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;31m# Step 9: Initialize the SFTTrainer, passing SFT-specific args here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustom_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 train_dataset = self._prepare_dataset(\n\u001b[0m\u001b[1;32m    363\u001b[0m                     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36m_prepare_dataset\u001b[0;34m(self, dataset, tokenizer, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens, skip_prepare_dataset)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpacking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             return self._prepare_non_packed_dataloader(\n\u001b[0m\u001b[1;32m    509\u001b[0m                 \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36m_prepare_non_packed_dataloader\u001b[0;34m(self, tokenizer, dataset, dataset_text_field, max_seq_length, formatting_func, add_special_tokens, remove_unused_columns)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0msignature_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mextra_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignature_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mremove_unused_columns\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_columns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'column_names'"],"ename":"AttributeError","evalue":"'list' object has no attribute 'column_names'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}